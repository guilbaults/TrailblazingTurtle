{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TrailblazingTurtle","text":""},{"location":"#introduction","title":"Introduction","text":"<p>TrailblazingTurtle is a web portal for HPC clusters. It is designed to be a single point of entry for users to access information about the cluster, their jobs, and the performance of the cluster. It is designed to be modular, so that it can be easily extended to support new features.</p>"},{"location":"#design","title":"Design","text":"<p>The Django portal will access various MySQL databases like the database of Slurm and Robinhood (if installed) to gather some information. </p> <p>Time series are stored with Prometheus for better performance. Compatible alternatives to Prometheus like Thanos, VictoriaMetrics, and Grafana Mimir should work without any problems (Thanos is used in production). Recorder rules in Prometheus are used to pre-aggregate some stats for the portal.</p> <p></p>"},{"location":"accountstats/","title":"Accountstats","text":"<p>The users can also see the aggregated use of the users in the same group. This also shows the current priority of this account in Slurm and a few months of history on how much computing resources were used.</p>"},{"location":"accountstats/#screenshots","title":"Screenshots","text":""},{"location":"accountstats/#account-stats","title":"Account stats","text":""},{"location":"accountstats/#requirements","title":"Requirements","text":"<ul> <li>Access to the database of Slurm</li> <li>slurm-job-exporter</li> </ul> <p>Optional:</p> <ul> <li>lustre_exporter and lustre_exporter_slurm (show Lustre information)</li> <li>slurm-exporter (show priority information)</li> </ul>"},{"location":"cfaccess/","title":"Cloudflare Access","text":"<p><code>cfaccess</code> is a optional feature of TrailblazingTurtle developed and contributed by ACENET for use on Siku.</p> <p>To use it on your own cluster, you will need a Cloudflare Account and Access Subscription.</p>"},{"location":"cfaccess/#how-it-works","title":"How it Works","text":"<p>Cloudflare Access provides a cloud reverse proxy which can authenticate the User before requests can hit your application server.</p> <p>When a request passes through Cloudflare Access, Cloudflare's reverse proxy will add a header (<code>Cf-Access-Jwt-Assertion</code>) and cookie (<code>CF_Authorization</code>) containing a JWT (Json Web Token). The JWT is composed of Base64 encoded JSON signed by Cloudflare and contains attributes about the authenticated user.</p> <p>Here is an example of the data encoded in the JWT:</p> <pre><code>{\n  \"aud\": [\n    \"32eafc7626e974616deaf0dc3ce63d7bcbed58a2731e84d06bc3cdf1b53c4228\"\n  ],\n  \"email\": \"user@example.com\",\n  \"exp\": 1659474457,\n  \"iat\": 1659474397,\n  \"nbf\": 1659474397,\n  \"iss\": \"https://yourteam.cloudflareaccess.com\",\n  \"type\": \"app\",\n  \"identity_nonce\": \"6ei69kawdKzMIAPF\",\n  \"sub\": \"7335d417-61da-459d-899c-0a01c76a2f94\",\n  \"country\": \"US\"\n}\n</code></pre> <p>You can learn more about the validation of the JWT here.</p> <p>Custom attributes from the identity provider can also be passed in the JWT, such as <code>eduPersonPrincipalName</code> or Compute Canada specific attributes such as <code>ccServiceAccess</code>, . You can learn more about the validation of the JWT here.</p>"},{"location":"cfaccess/#configuration","title":"Configuration","text":"<p>You must configure your application to be accessible to Cloudflare. Cloudflare provides Cloudflare Tunnels, a daemon which connects out to the Cloudflare Network and tunnels traffic back to your application, no Public IP address required.</p> <p>Configure your IDP(s) and Enable Cloudflare Access for your application.</p> <p>You will need to select a attribute from your IDP, which matches the username in LDAP. For Alliance IDP, you can pass <code>eduPersonPrincipalName</code>, which for alliance users will be <code>&lt;ldap uid&gt;@alliancecan.ca</code>.</p> <p>configuration your Cloudflare Access organization, Policy AUD, and Staff Attributes in <code>41-cloudflareaccess.py</code>.</p> <p>You must:</p> <ul> <li>Remove <code>djangosaml2.middleware.SamlSessionMiddleware</code> from <code>MIDDLEWARE</code></li> <li>Add <code>cfaccess.middleware.CloudflareAccessLDAPMiddleware</code> to <code>MIDDLEWARE</code></li> <li>Add <code>cfaccess.backends.CloudflareAccessLDAPBackend</code> to <code>AUTHENTICATION_BACKENDS</code></li> </ul>"},{"location":"cloudstats/","title":"Cloudstats","text":"<p>The stats of the VM running on Openstack can be viewed. This is using the stats of libvirtd, no agent needs to be installed in the VM. There is an overall stats page available for staff. The page per project and VM is also available for the users.</p>"},{"location":"cloudstats/#screenshots","title":"Screenshots","text":""},{"location":"cloudstats/#overall-use","title":"Overall use","text":""},{"location":"cloudstats/#use-within-a-project","title":"Use within a project","text":""},{"location":"cloudstats/#use-within-a-vm","title":"Use within a VM","text":""},{"location":"cloudstats/#requirements","title":"Requirements","text":"<ul> <li>libvirtd_exporter</li> </ul>"},{"location":"data/","title":"Data sources","text":"<p>The main requirement to monitor a Slurm cluster is to install slurm-job-exporter and open a read-only access to the Slurm MySQL database. Other data sources in this page can be installed to gather more data.</p>"},{"location":"data/#slurm-job-exporter","title":"slurm-job-exporter","text":"<p>slurm-job-exporter is used to capture information from cgroups managed by Slurm on each compute node. This gathers CPU, memory, and GPU utilization.</p> <p>The following recorder rules are used to pre-aggregate stats shown in the user portal.</p> <pre><code>---\ngroups:\n- name: recorder.rules\n  rules:\n  - record: slurm_job:allocated_core:count\n    expr: count(slurm_job_core_usage_total) by (cluster)\n  - record: slurm_job:allocated_core:count_user_account\n    expr: count(slurm_job_core_usage_total) by (user,account,cluster)\n  - record: slurm_job:used_core:sum\n    expr: sum(rate(slurm_job_core_usage_total{}[2m]) / 1000000000) by (cluster)\n  - record: slurm_job:used_core:sum_user_account\n    expr: sum(rate(slurm_job_core_usage_total{}[2m]) / 1000000000) by (user,account, cluster)\n  - record: slurm_job:allocated_memory:sum\n    expr: sum(slurm_job_memory_limit{}) by (cluster)\n  - record: slurm_job:allocated_memory:sum_user_account\n    expr: sum(slurm_job_memory_limit{}) by (user,account,cluster)\n  - record: slurm_job:rss_memory:sum\n    expr: sum(slurm_job_memory_rss) by (cluster)\n  - record: slurm_job:rss_memory:sum_user_account\n    expr: sum(slurm_job_memory_rss) by (user, account, cluster)\n  - record: slurm_job:max_memory:sum_user_account\n    expr: sum(slurm_job_memory_max) by (user, account, cluster)\n  - record: slurm_job:allocated_gpu:count\n    expr: count(slurm_job_utilization_gpu) by (cluster)\n  - record: slurm_job:allocated_gpu:count_user_account\n    expr: count(slurm_job_utilization_gpu) by (user, account, cluster)\n  - record: slurm_job:used_gpu:sum\n    expr: sum(slurm_job_utilization_gpu) by (cluster)/ 100\n  - record: slurm_job:used_gpu:sum_user_account\n    expr: sum(slurm_job_utilization_gpu) by (gpu_type,user,account,cluster) / 100\n  - record: slurm_job:non_idle_gpu:sum_user_account\n    expr: count(slurm_job_utilization_gpu &gt; 0) by (gpu_type,user,account,cluster)\n  - record: slurm_job:power_gpu:sum\n    expr: sum(slurm_job_power_gpu) by (gpu_type,cluster)\n  - record: slurm_job:power_gpu:sum_user_account\n    expr: sum(slurm_job_power_gpu) by (gpu_type,user,account,cluster)\n  - record: slurm_job:process_usage:sum_account\n    expr: sum(label_replace(deriv(slurm_job_process_usage_total{}[1m]) &gt; 0, \"bin\", \"$1\", \"exe\", \".*/(.*)\")) by (cluster, account, bin)\n</code></pre>"},{"location":"data/#access-to-the-database-of-slurmacct","title":"Access to the database of slurmacct","text":"<p>This MySQL database is accessed by a read-only user. It does not need to be in the same database server where Django is storing its data.</p>"},{"location":"data/#slurm-exporter","title":"slurm-exporter","text":"<p>slurm-exporter is used to capture stats from Slurm like the priority of each user. This portal is using a fork, branch <code>osc</code> in the linked repository. This fork support GPU reporting and sshare stats.</p>"},{"location":"data/#lustre_exporter-and-lustre_exporter_slurm","title":"lustre_exporter and lustre_exporter_slurm","text":"<p>Those 2 exporters are used to gather information about Lustre usage.</p> <ul> <li>lustre_exporter capture information on Lustre MDS and OSS but will only use \\$SLURM_JOBID as a tag on the metrics.</li> <li>lustre_exporter_slurm is used as a proxy between Prometheus and lustre_exporter to improve the content of the tags. This will match the \\$SLURM_JOBID to a job in Slurm and will add the username and Slurm account in the tags.</li> </ul> <p>The following recorder rules are used to pre-aggregate stats shown in the user portal.</p> <pre><code>---\ngroups:\n- name: recorder.rules\n  rules:\n  - record: lustre:read_bytes:rate3m\n    expr: sum(label_replace(rate(lustre_read_bytes_total{component=\"ost\"}[3m]), \"fs\", \"$1\", \"target\", \"(.*)-OST.*\")) by (fs, cluster)\n  - record: lustre:write_bytes:rate3m\n    expr: sum(label_replace(rate(lustre_write_bytes_total{component=\"ost\"}[3m]), \"fs\", \"$1\", \"target\", \"(.*)-OST.*\")) by (fs, cluster)\n  - record: lustre:read_bytes:rate3m_user\n    expr: sum by (user,fs,cluster,account) (rate(lustre_job_read_bytes_total{}[3m]))\n  - record: lustre:write_bytes:rate3m_user\n    expr: sum by (user,fs,cluster,account) (rate(lustre_job_write_bytes_total{}[3m]))\n  - record: lustre:metadata:rate3m\n    expr: sum(label_replace(rate(lustre_stats_total{component=\"mdt\"}[3m]), \"fs\", \"$1\", \"target\", \"(.*)-MDT.*\")) by (fs,operation,cluster)\n  - record: lustre:metadata:rate3m_user\n    expr: sum by (user,fs,cluster,account) (rate(lustre_job_stats_total{}[3m]))\n</code></pre>"},{"location":"data/#gpfs_exporter","title":"gpfs_exporter","text":"<p>gpfs_exporter is used to gather quotas from a GPFS filesystem. Learn more in quotasgpfs.md</p>"},{"location":"data/#redfish_exporter","title":"redfish_exporter","text":"<p>redfish_exporter is used to gather the power usage of the nodes. This information is used to compute the energy used by a job and related metrics like CO2 emissions.</p>"},{"location":"data/#node_exporter","title":"node_exporter","text":"<p>node_exporter is used to gather generic information about the nodes. This is the default exporter used by most Prometheus installations. That information is used to show metrics like the local disk IO of the nodes within the job.</p>"},{"location":"data/#libvirtd_exporter","title":"libvirtd_exporter","text":"<p>libvirtd_exporter is used to gather information about the VM running on Openstack.</p>"},{"location":"data/#pcm-sensor-server","title":"pcm-sensor-server","text":"<p>pcm-sensor-server is used to collect low-level data on Intel CPU like L2/L3 cache hit ratio, memory bandwidth, etc.</p>"},{"location":"data/#database-used-by-robinhood","title":"Database used by Robinhood","text":"<p>The information in this database is used to show the current utilization per user within a group.</p>"},{"location":"data/#slurm-jobscript","title":"Slurm jobscript","text":"<p>The script <code>slurm_jobscript/slurm_jobscripts.py</code> can be used to add the submitted script to the database of the portal. This should run on the Slurm server, it will collect the scripts from the <code>spool</code> directory of Slurm. This script uses the REST API of Django to push the job script. A user with a token need to be created, check the installation documentation on how to create this API token.</p>"},{"location":"development/","title":"Development","text":"<p>A test and development environment using the local <code>uid</code> resolver and dummies allocations is provided to test the portal.</p> <p>To use it, copy <code>example/local.py</code> to <code>userportal/local.py</code>. The other functions are documented in <code>common.py</code> if any other overrides are needed for your environment.</p> <p>To quickly test and bypass authentication, add this line to <code>userportal/settings/99-local.py</code>. Other local configuration can be added in this file to override the default settings.</p> <pre><code>AUTHENTICATION_BACKENDS.insert(0, 'userportal.authentication.staffRemoteUserBackend')\n</code></pre> <p>This bypasses the authentication and will use the <code>REMOTE_USER</code> header or env variable to authenticate the user. This is useful to be able to try the portal without having to set up a full IDP environment. The REMOTE_USER method can be used when using some IDP such as Shibboleth. SAML2 based IDP is now the preferred authentication method for production.</p> <p>Examine the default configuration in <code>userportal/settings/</code> and override any settings in <code>99-local.py</code> as needed.</p> <p>Then you can launch the example server with:</p> <pre><code>REMOTE_USER=someuser@alliancecan.ca affiliation=staff@alliancecan.ca python manage.py runserver\n</code></pre> <p>This will run the portal with the user <code>someuser</code> logged in as a staff member.</p> <p>Automated Django tests are also available, they can be run with:</p> <pre><code>python manage.py test\n</code></pre> <p>This will test the various modules, including reading job data from the Slurm database and Prometheus. A temporary database for Django is created automatically for the tests. Slurm and Prometheus data are read directly from production data with a read-only account. A representative user, job and account need to be defined to be used in the tests, check the <code>90-tests.py</code> file for an example.</p>"},{"location":"install/","title":"Installation","text":"<p>Before installing in production, a test environment should be set up to test the portal. This makes it easier to fully configure each module and modify as needed some functions like how the allocations are retrieved. Installing Prometheus and some exporters is also recommended to test the portal with real data.</p> <p>The portal can be installed directly on a Rocky8 Apache web server or with Nginx and Gunicorn. The portal can also be deployed as a container with Podman or Kubernetes. Some scripts used to deploy both Nginx and Django containers inside the same pod are provided in the <code>podman</code> directory. The various recommendations for any normal Django production deployment can be followed.</p> <p>Deploying Django</p> <p>The database should support UTF8. With MariaDB, the default collation can be changed with the following command:</p> <pre><code>ALTER DATABASE userportal CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n</code></pre> <p>Migration scripts will also ensure that the tables and columns are converted to the correct collation.</p>"},{"location":"install/#production-with-containers","title":"Production with containers","text":"<p>Using containers is the recommended way to deploy the portal. The container is automatically built in the CI pipeline and pushed to the Github registry. This container is handling the django application. The static files are served by a standard Nginx container. Both containers are deployed in the same pod with a shared volume containing the static files.</p>"},{"location":"install/#production-without-containers-on-rocky-linux-8","title":"Production without containers on Rocky Linux 8","text":"<p>RPMs required for production</p> <ul> <li><code>python39-pip</code> (python 3.9 will be used in the virtualenv)</li> <li><code>gcc</code> (to compile python modules)</li> <li><code>python3-virtualenv</code></li> <li><code>python39-mod_wsgi</code> or use Gunicorn instead of WSGI in Apache</li> <li><code>python39-devel</code></li> <li><code>openldap-devel</code></li> <li><code>mariadb-devel</code></li> <li><code>xmlsec1</code> (for SAML)</li> <li><code>xmlsec1-openssl</code> (for SAML)</li> </ul> <p>Python &gt;= 3.9 is required to fix a CVE in NumPy, you can install that version on Rocky8 inside a python virtualenv</p> <pre><code>/usr/bin/virtualenv-3 --python=\"/usr/bin/python3.9\" /var/www/userportal-env\n</code></pre> <p>A file in the python env needs to be patched, check the diff in <code>ldapdb.patch</code> if using the ldapdb module.</p> <p>Static files are handled by Apache and need to be collected since python will not serve them:</p> <pre><code>python manage.py collectstatic\n</code></pre>"},{"location":"install/#apache-virtualhost-config-for-wsgi","title":"Apache virtualhost config for WSGI","text":"<pre><code>&lt;VirtualHost *:443&gt;\n  ServerName userportal.int.ets1.calculquebec.ca\n\n  ## Vhost docroot\n  DocumentRoot \"/var/www/userportal\"\n  ## Alias declarations for resources outside the DocumentRoot\n  Alias /static \"/var/www/userportal-static\"\n  Alias /favicon.ico \"/var/www/userportal-static/favicon.ico\"\n\n  ## Directories, there should at least be a declaration for /var/www/userportal\n\n  &lt;Directory \"/var/www/userportal/\"&gt;\n    AllowOverride None\n    Require all granted\n  &lt;/Directory&gt;\n\n  &lt;Directory \"/var/www/userportal/static\"&gt;\n    AllowOverride None\n    Require all granted\n  &lt;/Directory&gt;\n\n  ## Logging\n  ErrorLog \"/var/log/httpd/userportal.int.ets1.calculquebec.ca_error_ssl.log\"\n  ServerSignature Off\n  CustomLog \"/var/log/httpd/userportal.int.ets1.calculquebec.ca_access_ssl.log\" combined\n\n  ## SSL directives\n  SSLEngine on\n  SSLCertificateFile      \"/etc/pki/tls/certs/userportal.int.ets1.calculquebec.ca.crt\"\n  SSLCertificateKeyFile   \"/etc/pki/tls/private/userportal.int.ets1.calculquebec.ca.key\"\n  WSGIApplicationGroup %{GLOBAL}\n  WSGIDaemonProcess userportal python-home=/var/www/userportal-env python-path=/var/www/userportal/\n  WSGIProcessGroup userportal\n  WSGIScriptAlias / \"/var/www/userportal/userportal/wsgi.py\"\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"install/#apache-virtualhost-config-for-gunicorn","title":"Apache virtualhost config for Gunicorn","text":"<p>In this example, Apache is running on port 8000 and Gunicorn on port 8001. Apache will handle the static files and forward the other requests to Gunicorn. An external load balancer is used to forward the requests from ports 80/443 to Apache on port 8000.</p> <pre><code>Listen 8000\n\n&lt;VirtualHost *:8000&gt;\n  DocumentRoot \"/var/www/userportal\"\n  Alias /static \"/var/www/userportal-static\"\n\n  ProxyPass / http://127.0.0.1:8001/\n  ProxyPassReverse / \"http://127.0.0.1:8001/\"\n\n  &lt;Directory \"/var/www/userportal/\"&gt;\n    AllowOverride None\n    Require all granted\n  &lt;/Directory&gt;\n\n  &lt;Directory \"/var/www/userportal/static\"&gt;\n    AllowOverride None\n    Require all granted\n  &lt;/Directory&gt;\n\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"install/#django-authentication","title":"Django Authentication","text":"<p>This portal is using the standard django authentication, multiple backends are supported, we are using SAML2 and the FreeIPA backend, on different clusters. Users with the is_staff attribute can access other users pages and the <code>top</code> module. </p>"},{"location":"install/#saml2","title":"SAML2","text":"<p>For SAML2, certificates need to be generated and the metadata.xml need a little modification.</p> <pre><code>openssl req -nodes -new -x509 -newkey rsa:2048 -days 3650 -keyout private.key -out public.cert\n</code></pre> <p>Download the metadata file from the IDP as metadata.xml Our Shibboleth IDP only seems to work with Redirect binding, so we manually remove the POST binding for SingleSignOnService.</p>"},{"location":"install/#api","title":"API","text":"<p>An API is available to modify resources in the database. This is used by the jobscript collector. A local superuser need to be created:</p> <pre><code>python manage.py createsuperuser\n</code></pre> <p>The token can be created with:</p> <pre><code>manage.py drf_create_token\n</code></pre>"},{"location":"install/#upgrades","title":"Upgrades","text":"<p>When SQL models are modified, the automated migration script needs to run once: <pre><code>python manage.py migrate\n</code></pre></p>"},{"location":"install/#translation","title":"Translation","text":"<p>Create the .po files for french: <code>python manage.py makemessages -l fr</code></p> <p>Update all message files for all languages: <pre><code>python manage.py makemessages -a\npython manage.py makemessages -a -d djangojs\n</code></pre></p> <p>Compile messages: <code>python manage.py compilemessages</code></p>"},{"location":"install/#deployment-with-kubernetes","title":"Deployment with Kubernetes","text":"<p>Kubernetes is used to serve the Django application. The production image is built with ubuntu and python3.8, Gunicorn is used to serve the application. The static files are served by Nginx. The image is built using the Containerfile in the root of the repository.</p> <p>To build the image manually, you can use the following command:</p> <pre><code>buildah bud -t userportal\n</code></pre> <p>Kaniko is used to build the image in the CI pipeline in Gitlab. The image is pushed to the registry and then deployed to the cluster. Some of the configuration files are located in the <code>kubernetes</code> directory.</p>"},{"location":"jobstats/","title":"Jobstats","text":"<p>Each user can see their current uses on the cluster and a few hours in the past. The stats for each job are also available. Information about CPU, GPU, memory, filesystem, InfiniBand, power, etc. is also available per job. The submitted job script can also be collected from the Slurm server and then stored and displayed in the portal. Some automatic recommendations are also given to the user, based on the content of their job script and the stats of their job.</p>"},{"location":"jobstats/#screenshots","title":"Screenshots","text":""},{"location":"jobstats/#user-stats","title":"User stats","text":""},{"location":"jobstats/#job-stats","title":"Job stats","text":""},{"location":"jobstats/#requirements","title":"Requirements","text":"<ul> <li>Access to the database of Slurm</li> <li>slurm-job-exporter</li> </ul> <p>Optional:</p> <ul> <li>node_exporter (show node information)</li> <li>redfish_exporter (show power information)</li> <li>lustre_exporter and lustre_exporter_slurm (show Lustre information)</li> <li>slurm_jobscripts.py (show the submitted jobscript)</li> <li>pcm-sensor-server from Intel PCM (show CPU information like memory bandwidth, cache misses, etc.)</li> </ul>"},{"location":"nodes/","title":"Nodes","text":"<p>This main page present the list of nodes in the cluster with a small graph representing the cores, memory and localdisk used. Each node has a link to a detailed page with more information about the node similar to the jobstats page.</p>"},{"location":"nodes/#screenshots","title":"Screenshots","text":""},{"location":"nodes/#nodes-list","title":"Nodes list","text":""},{"location":"nodes/#node-details","title":"Node details","text":""},{"location":"nodes/#requirements","title":"Requirements","text":"<ul> <li>Access to the database of Slurm</li> <li>slurm-job-exporter</li> <li>node_exporter</li> </ul>"},{"location":"quotas/","title":"Quotas","text":"<p>Each user can see their current storage allocations and who within their group is using the group quota.</p>"},{"location":"quotas/#screenshots","title":"Screenshots","text":""},{"location":"quotas/#quotas_1","title":"Quotas","text":""},{"location":"quotas/#hsm","title":"HSM","text":""},{"location":"quotas/#requirements","title":"Requirements","text":"<ul> <li>Read-only access to the databases of Robinhood</li> </ul>"},{"location":"quotasgpfs/","title":"quotasgpfs","text":"<p><code>quotasgpfs</code> is a optional module for TrailblazingTurtle, developed by ACENET for use on their Siku cluster.</p> <p>To enable, you must add <code>quotasgpfs</code> to <code>INSTALLED_APPS</code></p>"},{"location":"quotasgpfs/#gpfs_exporter","title":"gpfs_exporter","text":"<p><code>quotasgpfs</code> uses data from gpfs_exporter's <code>mmrepquota</code> module. Use the <code>--collector.mmrepquota.quota-types</code> option to enable collection of user and group quotas. It is assumed that quota names are uid/gid numbers.</p>"},{"location":"top/","title":"Top","text":"<p>These pages are only available to staff and are meant to visualize poor cluster utilization:</p> <ul> <li>Largest compute users, CPU cores, and GPUs</li> <li>Jobs on large memory nodes (ranked by worst to best)</li> <li>Top users on Lustre</li> </ul>"},{"location":"top/#screenshots","title":"Screenshots","text":""},{"location":"top/#top-compute-user-cpu","title":"Top compute user (CPU)","text":""},{"location":"top/#top-compute-user-gpu","title":"Top compute user (GPU)","text":""},{"location":"top/#jobs-on-large-memory-nodes","title":"Jobs on large memory nodes","text":""},{"location":"top/#top-users-on-lustre","title":"Top users on Lustre","text":""},{"location":"top/#requirements","title":"Requirements","text":"<ul> <li>Access to the database of Slurm</li> <li>slurm-job-exporter</li> </ul> <p>Optional:</p> <ul> <li>lustre_exporter and lustre_exporter_slurm (show Lustre information)</li> </ul>"},{"location":"usersummary/","title":"User Summary","text":"<p>The usersummary page can be used for a quick diagnostic of a user to see their current quotas and last jobs.</p>"},{"location":"usersummary/#screenshots","title":"Screenshots","text":""},{"location":"usersummary/#quotas-and-jobs-of-a-user","title":"Quotas and jobs of a user","text":""},{"location":"usersummary/#requirements","title":"Requirements","text":"<ul> <li>Access to the database of Slurm</li> <li>slurm-job-exporter</li> </ul>"}]}